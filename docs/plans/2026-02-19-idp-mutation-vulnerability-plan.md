# IDP Mutation Vulnerability Framework — Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Build a mutation-level pathogenicity predictor for IDP-related genes using IDP-specific features (maturation grammar, condensate physics, amyloid propensity), and compare three modeling approaches.

**Architecture:** Shared data pipeline (ClinVar download → feature engineering) feeds three parallel modeling approaches (handcrafted features, IDR-stratified, ESM2 embeddings). Evaluated via leave-one-gene-out cross-validation. ~2,100 pathogenic + ~8,000 benign missense variants across 23 genes.

**Tech Stack:** Python 3.12, scikit-learn, XGBoost, PyTorch + HuggingFace transformers (ESM2), metapredict (disorder), pandas, matplotlib. Existing maturation grammar from condensate-maturation-theory project.

**Project root:** `/storage/kiran-stuff/idp-mechanism-classifier/`

**Existing reusable code:**
- `scripts/03_maturation_grammar.py` — SequenceGrammar wrapper (p_lock, p_pi, p_polar per window)
- `scripts/04_amyloid_propensity.py` — APR detection, β-propensity scoring
- `scripts/05_esm2_embeddings.py` — ESM2 via HuggingFace transformers (GPU)

---

### Task 1: Install dependencies and create project structure

**Files:**
- Create: `scripts/mutation/` directory
- Create: `data/` directory (for ClinVar download)
- Create: `data/variants/` directory (for processed variants)

**Step 1: Install missing packages**

Run:
```bash
pip install xgboost metapredict
```

**Step 2: Create directory structure**

Run:
```bash
mkdir -p /storage/kiran-stuff/idp-mechanism-classifier/scripts/mutation
mkdir -p /storage/kiran-stuff/idp-mechanism-classifier/data/variants
mkdir -p /storage/kiran-stuff/idp-mechanism-classifier/data/sequences
mkdir -p /storage/kiran-stuff/idp-mechanism-classifier/data/disorder
```

**Step 3: Verify installs**

Run:
```bash
python3 -c "import xgboost; import metapredict; print('OK')"
```

---

### Task 2: Download and parse ClinVar variant data

**Files:**
- Create: `scripts/mutation/01_download_clinvar.py`
- Output: `data/variants/clinvar_idp_missense.csv`

**Purpose:** Download ClinVar variant_summary.txt.gz, filter for our 23 genes, extract missense variants with protein positions.

**Step 1: Write the download/parse script**

The script should:
1. Download `variant_summary.txt.gz` from ClinVar FTP (if not already cached)
2. Filter rows where `GeneSymbol` matches our 23 genes
3. Filter for `Type == "single nucleotide variant"`
4. Parse protein change from the `Name` field using regex: `p\.([A-Z][a-z]{2})(\d+)([A-Z][a-z]{2})`
5. Convert 3-letter AA codes to 1-letter
6. Filter: keep only rows where we successfully extracted a protein position
7. Assign label: `pathogenic` (P + LP), `benign` (B + LB), `vus` (VUS)
8. Filter by ReviewStatus: keep ≥1 star (exclude "no assertion criteria provided")
9. Remove duplicate positions (same gene + position + alt AA → keep highest confidence label)
10. Save to CSV: gene, uniprot_id, position, ref_aa, alt_aa, label, clinical_significance, review_status, condition

**Gene list (23):** SNCA, APP, MAPT, TARDBP, FUS, HNRNPA1, PRNP, HTT, TIA1, EWSR1, TAF15, HNRNPA2B1, ATXN3, DDX4, NPM1, IAPP, SOD1, TTR, LMNA, VCP, AR, SQSTM1, CRYAB

**Step 2: Run the script**

Run: `python3 scripts/mutation/01_download_clinvar.py`

**Step 3: Verify output**

Check: row counts per gene, per label. Expected: ~2,000+ pathogenic, ~5,000+ benign.
Spot-check known mutations: SNCA A53T should be pathogenic, MAPT P301L should be pathogenic.

---

### Task 3: Fetch UniProt sequences for all 23 genes

**Files:**
- Create: `scripts/mutation/02_fetch_sequences.py`
- Output: `data/sequences/all_proteins.fasta`, `data/sequences/protein_info.csv`

**Purpose:** Get canonical UniProt sequences for all 23 genes. We need these to: (a) validate ClinVar protein positions, (b) compute local window features.

**UniProt IDs:**
- SNCA: P37840, APP: P05067, MAPT: P10636, TARDBP: Q13148
- FUS: P35637, HNRNPA1: P09651, PRNP: P04156, HTT: P42858
- TIA1: P31483, EWSR1: Q01844, TAF15: Q92804, HNRNPA2B1: P22626
- ATXN3: P54252, DDX4: Q9NQI0, NPM1: P06748, IAPP: P10997
- SOD1: P00441, TTR: P02766, LMNA: P02545, VCP: P55072
- AR: P10275, SQSTM1: Q13501, CRYAB: P02511

**Step 1: Write the fetch script**

Use requests to fetch FASTA from UniProt REST API for each protein. Save combined FASTA and a CSV with (gene, uniprot_id, length, description).

**Step 2: Run and verify**

Run: `python3 scripts/mutation/02_fetch_sequences.py`
Check: 23 sequences, lengths match expected (e.g., SNCA=140, MAPT=758 for canonical).

**Step 3: Cross-validate ClinVar positions**

For each ClinVar variant, check that the ref_aa at the stated position matches the UniProt sequence. Flag mismatches (likely isoform issues). Report mismatch rate.

---

### Task 4: Compute disorder predictions

**Files:**
- Create: `scripts/mutation/03_disorder_prediction.py`
- Output: `data/disorder/per_residue_disorder.csv` (gene, position, disorder_score)

**Purpose:** Per-residue disorder probability for all 23 proteins. Used to annotate each variant position as IDR or structured.

**Step 1: Write script using metapredict**

```python
import metapredict as meta
# for each protein sequence:
disorder_scores = meta.predict_disorder(sequence)
# scores are per-residue, 0-1 (>0.5 = disordered)
```

**Step 2: Run and verify**

Run: `python3 scripts/mutation/03_disorder_prediction.py`
Check: SNCA should be ~100% disordered. TARDBP RRM domains should be ordered, LCD should be disordered.

---

### Task 5: Compute local window features at each variant position

**Files:**
- Create: `scripts/mutation/04_local_features.py`
- Input: `data/variants/clinvar_idp_missense.csv`, `data/sequences/all_proteins.fasta`, `data/disorder/per_residue_disorder.csv`
- Output: `data/variants/local_features.csv`

**Purpose:** For each variant position, compute features in a ±15 residue window around the mutation site.

**Features to compute (per position):**
1. `local_hydrophobicity` — mean Kyte-Doolittle in window
2. `local_charge_density` — fraction of D,E,K,R in window
3. `local_aromatic_density` — fraction of F,W,Y in window
4. `local_glycine_frac` — fraction of G in window
5. `local_proline_frac` — fraction of P in window
6. `local_beta_propensity` — mean Chou-Fasman β-sheet propensity in window
7. `local_disorder` — mean disorder score in window (from Task 4)
8. `position_disorder` — disorder score at the exact mutation position
9. `local_qn_frac` — fraction Q+N in window
10. `relative_position` — position / protein_length (0-1)

**Maturation grammar at position** (reuse existing SequenceGrammar):
11. `local_p_lock` — grammar p_lock score at the window containing the mutation
12. `local_p_pi` — grammar p_pi at that window
13. `local_p_polar` — grammar p_polar at that window

**Amyloid features at position** (reuse existing APR detection):
14. `in_apr` — binary: is the mutation inside an aggregation-prone region?
15. `local_amyloid_propensity` — mean amyloid propensity in window
16. `is_sticker` — binary: is the position a Tyr/Phe/Trp (aromatic sticker)?

**Step 1: Write the script**

Import grammar code from `/storage/kiran-stuff/condensate-maturation-theory/condensate_maturation/`. Reuse APR detection logic from `scripts/04_amyloid_propensity.py`.

**Step 2: Run**

Run: `python3 scripts/mutation/04_local_features.py`

**Step 3: Verify**

Check SNCA position 53 (A53T): should show moderate amyloid propensity (in NAC-adjacent region). Check FUS position 525 (P525L): should show low grammar scores (outside LCD). Check hnRNPA1 position 262 (D262V): should show high grammar scores (inside PrLD).

---

### Task 6: Compute mutation-specific features

**Files:**
- Create: `scripts/mutation/05_mutation_features.py`
- Output: `data/variants/mutation_features.csv`

**Purpose:** Features that describe the nature of the amino acid change itself.

**Features:**
1. `blosum62` — BLOSUM62 substitution score (ref→alt)
2. `grantham_distance` — Grantham physicochemical distance
3. `charge_change` — categorical: neutral→neutral, neutral→charged, charged→neutral, charge_flip
4. `hydrophobicity_change` — ΔKyte-Doolittle (alt - ref)
5. `size_change` — Δmolecular weight (alt - ref)
6. `creates_proline` — binary: does the mutation introduce a proline (β-breaker)?
7. `destroys_proline` — binary: does the mutation remove a proline?
8. `creates_glycine` — binary: introduces glycine (flexibility)?
9. `aromatic_change` — categorical: gains aromatic, loses aromatic, no change
10. `polarity_change` — categorical: polar→nonpolar, nonpolar→polar, no change

**Step 1: Write the script**

Use standard BLOSUM62 matrix from Biopython. Grantham distances from published table. AA properties from standard references.

**Step 2: Run and verify**

Check: E46K in SNCA → charge_change = "charge_flip" (negative→positive), large Grantham distance.
Check: A53T in SNCA → polarity_change = "nonpolar→polar".

---

### Task 7: Compute regional and protein-level features

**Files:**
- Create: `scripts/mutation/06_regional_features.py`
- Output: `data/variants/regional_features.csv`

**Purpose:** Annotate each variant position with structural/functional region info and protein-level context.

**Regional features:**
1. `in_idr` — binary: disorder_score > 0.5
2. `in_lcd` — binary: position falls within known LCD/PrLD (from our existing annotations + SEG-like detection)
3. `in_functional_domain` — binary: position is in annotated Pfam/InterPro domain
4. `domain_type` — categorical: NLS, RRM, zinc_finger, PrLD, polyQ_tract, signal_peptide, none
5. `idr_length` — length of the IDR containing this position (0 if in structured region)

**Protein-level features (same for all variants in a gene):**
6. `protein_length` — total protein length
7. `total_disorder_fraction` — fraction of protein predicted disordered
8. `n_lcds` — number of LCD regions
9. `mechanism_class` — from our existing classification (amyloid, condensate, etc.)

**Data sources:**
- Disorder: from Task 4
- LCD regions: extend existing LCD_REGIONS dict from `03_maturation_grammar.py` to cover all 23 proteins
- Domain annotations: fetch from UniProt feature annotations (via REST API)

**Step 1: Write the script**

Fetch InterPro/Pfam domain annotations from UniProt API for each protein. Map to domain_type categories.

**Step 2: Run and verify**

Check: FUS position 525 should have domain_type = "NLS". TARDBP position 337 should be in_lcd = True. SNCA position 53 should be in_idr = True.

---

### Task 8: Compute ESM2 per-residue features

**Files:**
- Create: `scripts/mutation/07_esm2_per_residue.py`
- Output: `data/variants/esm2_features.csv`

**Purpose:** For each variant position, compute ESM2 log-likelihood ratio (WT vs mutant) and local embedding features. This is for Approach C and as a feature in A/B.

**Features:**
1. `esm2_llr` — log P(ref_aa | context) - log P(alt_aa | context) at the position
2. `esm2_embedding_norm` — L2 norm of the ESM2 residue embedding at the position
3. `esm2_local_embedding_var` — variance of embeddings in ±5 window (captures local "surprise")

**Implementation:**
- Load ESM2-650M via HuggingFace transformers (already working in `05_esm2_embeddings.py`)
- For each protein: run through model, extract per-residue hidden states and masked LM logits
- Cache the full per-residue outputs per protein (avoid re-running for each variant)
- GPU required (use GPU 1)

**Step 1: Write the script**

Run ESM2 once per protein (23 forward passes), cache per-residue embeddings and logits. Then for each variant, extract features from the cached outputs.

**Step 2: Run on GPU**

Run: `CUDA_VISIBLE_DEVICES=1 python3 scripts/mutation/07_esm2_per_residue.py`
Expected: ~5-10 minutes (23 proteins, some large like LMNA=664aa, AR=919aa).

---

### Task 9: Merge all features into final training matrix

**Files:**
- Create: `scripts/mutation/08_merge_features.py`
- Output: `data/variants/training_matrix.csv`

**Purpose:** Combine all feature CSVs into one training matrix with labels.

**Columns:**
- Identifiers: gene, position, ref_aa, alt_aa, label (pathogenic/benign)
- Local features (Task 5): 16 columns
- Mutation features (Task 6): 10 columns
- Regional features (Task 7): 9 columns
- ESM2 features (Task 8): 3 columns
- Total: ~38 feature columns + identifiers + label

**Step 1: Write the merge script**

Inner join all feature CSVs on (gene, position, ref_aa, alt_aa). Drop rows with any NaN. Report final row count per gene and per label.

**Step 2: Run and verify**

Run: `python3 scripts/mutation/08_merge_features.py`
Check: no NaN values. Expected: ~5,000-10,000 rows. Print class balance per gene.

---

### Task 10: Approach A — XGBoost on handcrafted features

**Files:**
- Create: `scripts/mutation/09_approach_a_xgboost.py`
- Output: `data/variants/results_approach_a.csv`, `figures/approach_a_*.png`

**Purpose:** Train XGBoost classifier on all handcrafted features (~35) with leave-one-gene-out cross-validation.

**Implementation:**
1. Load training_matrix.csv
2. Define feature columns (exclude ESM2 embedding features — those are for Approach C)
3. Leave-one-gene-out CV:
   - For each gene g: train on all other genes, predict on g
   - Collect predictions for all variants
4. Compute metrics: overall AUROC, AUPRC, per-gene AUROC
5. Feature importance: mean SHAP values or XGBoost feature_importances_ across folds
6. Baselines: ESM2 LLR alone, disorder score alone, random

**XGBoost hyperparameters:**
```python
params = {
    "n_estimators": 500,
    "max_depth": 6,
    "learning_rate": 0.05,
    "subsample": 0.8,
    "colsample_bytree": 0.8,
    "scale_pos_weight": n_benign / n_pathogenic,  # handle class imbalance
    "eval_metric": "logloss",
    "random_state": 42,
}
```

**Figures:**
- ROC curve (overall + per-gene)
- Feature importance bar plot
- Calibration plot

---

### Task 11: Approach B — IDR vs Structured stratification

**Files:**
- Create: `scripts/mutation/10_approach_b_stratified.py`
- Output: `data/variants/results_approach_b.csv`, `figures/approach_b_*.png`

**Purpose:** Same XGBoost as Approach A, but with post-hoc analysis:
1. Train full model (same as A)
2. Separately evaluate performance on IDR variants vs structured variants
3. Train separate models for IDR-only and structured-only subsets
4. Compare feature importances between the two

**Key questions:**
- Does the model perform better on IDR or structured variants?
- Do grammar/condensate features rank higher in IDR variants?
- Does the IDR-specific model outperform AlphaMissense on IDR variants?

---

### Task 12: Approach C — ESM2 embeddings

**Files:**
- Create: `scripts/mutation/11_approach_c_esm2.py`
- Output: `data/variants/results_approach_c.csv`, `figures/approach_c_*.png`

**Purpose:** Use ESM2 per-residue embeddings as primary features.

**Implementation options (try both):**
1. **ESM2 PCA + XGBoost**: Take ESM2 1280-dim embedding at mutation position, PCA to 50 components, concatenate with handcrafted features, XGBoost with LOGO-CV
2. **ESM2 + MLP**: 2-layer MLP (1280 → 256 → 64 → 1) on ESM2 embedding at mutation position, optionally concatenated with handcrafted features. LOGO-CV.

Same evaluation: AUROC, AUPRC, per-gene, feature contribution analysis.

---

### Task 13: Compare all approaches

**Files:**
- Create: `scripts/mutation/12_compare_approaches.py`
- Output: `figures/approach_comparison.png`, `data/variants/comparison_summary.csv`

**Purpose:** Head-to-head comparison of A, B, C and baselines.

**Comparison table:**
| approach | AUROC | AUPRC | min per-gene AUROC | n features | interpretable? |
|----------|-------|-------|-------------------|-----------|---------------|
| A: XGBoost handcrafted | | | | ~35 | yes |
| B: XGBoost IDR-only | | | | ~35 | yes |
| B: XGBoost structured-only | | | | ~35 | yes |
| C: ESM2 PCA + XGBoost | | | | ~85 | partial |
| C: ESM2 MLP | | | | 1280 | no |
| baseline: ESM2 LLR alone | | | | 1 | yes |
| baseline: disorder alone | | | | 1 | yes |
| baseline: random | 0.50 | | | 0 | n/a |

**Figures:**
- Multi-ROC overlay plot
- Per-gene AUROC heatmap (approach × gene)
- Feature importance comparison (A vs B-IDR vs B-structured)

---

### Task 14: Generate publication figures and final analysis

**Files:**
- Create: `scripts/mutation/13_publication_figures.py`
- Output: `figures/fig_mutation_*.png`

**Purpose:** Final publication-quality figures.

**Planned figures:**
1. **Data overview**: variant counts per gene, colored by label, with IDR annotation
2. **ROC comparison**: approaches A, B, C + baselines
3. **Feature importance**: top 15 features from best approach, colored by feature category
4. **IDR vs structured**: split violin plot showing feature importance differences
5. **Per-gene performance**: heatmap of AUROC by gene × approach
6. **Example predictions**: selected well-known mutations positioned in feature space

---

### Task 15: Write results summary and update PROJECT.md

**Files:**
- Modify: `PROJECT.md`
- Create: `MUTATION_ANALYSIS.md`

**Purpose:** Document findings, update project with mutation-level results, assess publishability.

---

## Execution order and dependencies

```
Task 1 (setup)
    ↓
Task 2 (ClinVar) → Task 3 (UniProt) → Task 4 (disorder)  [can parallelize 2-4]
    ↓                   ↓                    ↓
    +-------------------+--------------------+
    ↓
Task 5 (local features) ← needs sequences + disorder + variants
Task 6 (mutation features) ← needs variants only
Task 7 (regional features) ← needs sequences + disorder
Task 8 (ESM2 per-residue) ← needs sequences  [GPU, can run in parallel with 5-7]
    ↓
Task 9 (merge all)
    ↓
Task 10 (approach A) → Task 11 (approach B) → Task 12 (approach C)  [can parallelize]
    ↓                      ↓                       ↓
Task 13 (compare)
    ↓
Task 14 (figures) → Task 15 (writeup)
```

Tasks 2-4 are independent and can run in parallel.
Tasks 5-8 can partially overlap (5-7 need output from 2-4; 8 only needs sequences).
Tasks 10-12 are independent and can run in parallel after Task 9.
